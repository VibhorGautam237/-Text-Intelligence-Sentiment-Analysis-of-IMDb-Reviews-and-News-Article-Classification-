{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f936003-e2f9-4c28-b5d5-67f86e0ad166",
   "metadata": {},
   "source": [
    "# Part A: IMDb Movie Review Sentiment Analysis\n",
    "# OVERVIEW\n",
    "The \"Overview\" section describes sentiment analysis as a Natural Language Processing (NLP) task focused on determining wheather a given text expresses positive or negatie sentimeny.Analyzemovie reviews from the IMDb datasets to predict sentiment (positive or negative) based on the text.Develop a machine learning model capable of accurately predicting movie review sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4cd5e3-7e96-492c-83fc-115600186bc6",
   "metadata": {},
   "source": [
    "# PROBLEM STATEMENT\n",
    "The primary objective of this project is to develop a machine learning classification model that accurately predicts the sentiment of IMDb movie reviews. the dataset contains a collection of movie reviews, and each review is labeled as either positive or negative. using text preprocessing (such as TF-IDF),and various classification algorithms, the project will aim to develop a model that can effective classify the sentiment of movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1390c9-be4c-4cf7-9b76-dc7b45588dcc",
   "metadata": {},
   "source": [
    "# Information about structure of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a3237b8-0ee1-4a9c-8a54-bc19d25231fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef895bb1-6804-4d41-8c93-80c677459494",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\india\\\\Downloads\\\\Imdb data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed6c81e-4185-443d-85c8-fac5287c26ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n",
      "review       object\n",
      "sentiment    object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e03ad-8884-48fe-b169-dff7c6f4152d",
   "metadata": {},
   "source": [
    "#  Dataset Information:\n",
    "The IMDb dataset contains a large number of movie reviews, each labeled with either a positive or negative sentiment..\n",
    "● Text of the review: The actual review provided by the user.\n",
    "● Sentiment label: The sentiment of the review, either \"positive\" or \"negative.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d80f89-8632-49d6-9e04-407d21e810ba",
   "metadata": {},
   "source": [
    "#  1. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615e1c4-824e-4058-bb78-c181725e1414",
   "metadata": {},
   "source": [
    "# ● Analyze the dataset for trends, missing values, and outliers.\n",
    "o Perform basic data exploration, such as checking for missing values, identifying imbalanced classes (positive/negative), and analyzing the length of\n",
    " reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17bab58c-f6f4-467b-9402-ce9af0f64545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "\n",
      "No 'sentiment_column' found for class imbalance analysis.\n",
      "\n",
      "No 'review_text_column' found for review length analysis.\n",
      "\n",
      "No 'numerical_column' found for outlier analysis example.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'df' is your DataFrame, replace with your actual data loading\n",
    "# Example: df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# --- 1. Analyze for missing values ---\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Optional: Visualize missing values (e.g., using missingno library)\n",
    "# import missingno as msno\n",
    "# msno.matrix(df)\n",
    "# msno.bar(df)\n",
    "\n",
    "# --- 2. Identify imbalanced classes (positive/negative) ---\n",
    "# This assumes you have a 'sentiment' or similar column indicating positive/negative classes\n",
    "# Replace 'sentiment_column' with the actual column name\n",
    "if 'sentiment_column' in df.columns:\n",
    "    print(\"\\nClass distribution of 'sentiment_column':\")\n",
    "    print(df['sentiment_column'].value_counts(normalize=True))\n",
    "else:\n",
    "    print(\"\\nNo 'sentiment_column' found for class imbalance analysis.\")\n",
    "\n",
    "# --- 3. Analyze the length of reviews ---\n",
    "# This assumes you have a 'review_text' or similar column containing review content\n",
    "# Replace 'review_text_column' with the actual column name\n",
    "if 'review_text_column' in df.columns:\n",
    "    df['review_length'] = df['review_text_column'].apply(len)\n",
    "    print(\"\\nDescriptive statistics for review length:\")\n",
    "    print(df['review_length'].describe())\n",
    "\n",
    "    # Optional: Visualize review length distribution\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # import seaborn as sns\n",
    "    # sns.histplot(df['review_length'], bins=50)\n",
    "    # plt.title('Distribution of Review Lengths')\n",
    "    # plt.xlabel('Review Length')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.show()\n",
    "else:\n",
    "    print(\"\\nNo 'review_text_column' found for review length analysis.\")\n",
    "\n",
    "# --- 4. Analyze for trends and outliers (General approach) ---\n",
    "# Trends and outliers analysis depends heavily on the specific dataset and columns.\n",
    "# Below are general examples for numerical columns.\n",
    "\n",
    "# Example: Detecting outliers in a numerical column using IQR\n",
    "# Replace 'numerical_column' with an actual numerical column name\n",
    "if 'numerical_column' in df.columns:\n",
    "    Q1 = df['numerical_column'].quantile(0.25)\n",
    "    Q3 = df['numerical_column'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df['numerical_column'] < lower_bound) | (df['numerical_column'] > upper_bound)]\n",
    "    print(f\"\\nNumber of outliers in 'numerical_column': {len(outliers)}\")\n",
    "    # print(outliers) # Uncomment to see the outlier rows\n",
    "else:\n",
    "    print(\"\\nNo 'numerical_column' found for outlier analysis example.\")\n",
    "\n",
    "# Example: Basic trend observation (e.g., if you have a time-series component)\n",
    "# If your data has a time component, you'd typically resample and plot to see trends.\n",
    "# This example is illustrative and requires a datetime index or column.\n",
    "# df['date_column'] = pd.to_datetime(df['date_column']) # Convert to datetime\n",
    "# df.set_index('date_column', inplace=True)\n",
    "# df['value_column'].resample('M').mean().plot() # Monthly average trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1d936-e664-461e-8e6c-f497b9bb6bcd",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "This Python code snippet provides a basic framework for performing the data exploration tasks outlined in your problem statement using the pandas library, which is fundamental for data manipulation and analysis in Python.\n",
    " 1. Missing Values = df.isnull().sum(): This line directly identifies and sums the number of missing (NaN) values for each column in your DataFrame (df). This helps you understand which columns have missing data and to what extent.\n",
    "2. Imbalanced Classes = df['sentiment_column'].value_counts(normalize=True): This code checks the distribution of classes within a specified categorical column (e.g., 'sentiment_column' for positive/negative reviews). normalize=True shows the proportion of each class, making it easy to spot imbalances.\n",
    "3. Length of Reviews = df['review_length'] = df['review_text_column'].apply(len): A new column, review_length, is created by applying the len() function to each entry in your text-based review column.\n",
    "   df['review_length'].describe(): This provides descriptive statistics (mean, min, max, quartiles) for the review lengths, giving insights into the typical length and spread.\n",
    "4. Trends and Outliers\n",
    "  + Outliers (IQR Method) = This example demonstrates a common method for detecting outliers in a numerical column using the Interquartile Range (IQR). Values falling significantly outside the Q1 - 1.5\\*IQR and Q3 + 1.5\\*IQR range are considered outliers.\n",
    "  + Trends = Analyzing trends typically involves time-series data. If your dataset includes a time component, you would often convert a date/time column to a datetime object, set it as the index, and then use resampling techniques (e.g., resample('M').mean()) to observe patterns over time. The provided code includes a commented-out example for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620adf0-f3a6-4bff-86fa-eaa7bcb3beae",
   "metadata": {},
   "source": [
    "#  ● Perform data cleaning and text preprocessing.\n",
    "#  o Stepswill include:\n",
    "# ▪ Removing stop words, punctuation, and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f65ac31-475d-48ce-af0d-a960885c95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Example usage:\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# text = \"This is an example sentence, with some punctuation and numbers like 123.\"\n",
    "# cleaned_text = clean_text(text)\n",
    "# print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d07d9e-98e1-47ee-b8e0-2a928deb535d",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "This function clean_text uses the re module for regular expressions to remove special characters and string.punctuation to remove punctuation. It also utilizes nltk.corpus.stopwords to remove common English stop words, filtering them out from the text after splitting it into words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5bf409-0a98-4772-ad40-aa4041d56c07",
   "metadata": {},
   "source": [
    "# ▪ Tokenization of text (splitting text into words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3815e28a-0eac-4d50-a084-58500c52f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Example usage:\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# text = \"This is an example sentence.\"\n",
    "# tokens = tokenize_text(text)\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a449b30d-a3c0-48ef-a17e-874670536455",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "The tokenize_text function employs nltk.tokenize.word_tokenize to split the input text into individual words or tokens, which is a fundamental step in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d355bc4b-5c03-4000-84a7-13ce27daa757",
   "metadata": {},
   "source": [
    "#  ▪ Lemmatization and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7fee0a0-62c8-4402-9769-99419cbd7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "def lemmatize_and_stem(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return lemmas, stems\n",
    "\n",
    "# Example usage:\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# tokens = ['running', 'runs', 'ran']\n",
    "# lemmas, stems = lemmatize_and_stem(tokens)\n",
    "# print(\"Lemmas:\", lemmas)\n",
    "# print(\"Stems:\", stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238584a-f81d-4f77-a13d-c82eae36c38e",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "This function lemmatize_and_stem performs both lemmatization and stemming. It uses nltk.stem.WordNetLemmatizer to reduce words to their base form (lemma) and nltk.stem.PorterStemmer to reduce words to their root form (stem), which helps in normalizing text for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0f5aa-3048-4417-a33d-501f244ead98",
   "metadata": {},
   "source": [
    "#  ▪ Vectorization using techniques like Bag-of-Words and TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f72f703f-bb2b-4736-9187-2676340cb146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def vectorize_text(corpus):\n",
    "    # Bag-of-Words\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    bow_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    return bow_matrix, tfidf_matrix, count_vectorizer, tfidf_vectorizer\n",
    "\n",
    "# Example usage:\n",
    "# corpus = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "# bow_matrix, tfidf_matrix, count_vectorizer, tfidf_vectorizer = vectorize_text(corpus)\n",
    "# print(\"Bag-of-Words Matrix:\\n\", bow_matrix.toarray())\n",
    "# print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "# print(\"Bag-of-Words Feature Names:\", count_vectorizer.get_feature_names_out())\n",
    "# print(\"TF-IDF Feature Names:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801bba0-b506-40ad-9043-ee6f6c75e541",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "The vectorize_text function demonstrates two common vectorization techniques: Bag-of-Words and TF-IDF. It utilizes sklearn.feature_extraction.text.CountVectorizer to create a matrix representing the frequency of words in documents and sklearn.feature_extraction.text.TfidfVectorizer to create a matrix where word importance is weighted by their frequency in a document relative to their frequency across all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496dbc2-5cc9-4705-b4a8-1f55b7ec642e",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering:\n",
    "# ● Feature extraction using techniques like TF-IDF, Word2Vec, or embeddings.\n",
    "#    o Transform the textual data into numerical features that can be used by machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dba139d-5fe5-4168-87da-441caed80c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      " [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "\n",
      "Feature Names (Vocabulary):\n",
      " ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "# 1. TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the TF-IDF matrix (sparse matrix)\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n",
    "\n",
    "# Print the feature names (words)\n",
    "print(\"\\nFeature Names (Vocabulary):\\n\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac705730-8b05-4c0d-ad50-82a5ef5f0a64",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "+ TfidfVectorizer from sklearn.feature_extraction.text is used to convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "+ fit_transform() calculates the TF-IDF values for each word in each document and returns a sparse matrix.\n",
    "+ toarray() converts the sparse matrix to a dense NumPy array for easier viewing.\n",
    "+ get_feature_names_out() retrieves the vocabulary learned by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc27ece-ca4c-4bb4-8bb6-8d8a96ef2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Word2Vec:\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data (if not already downloaded)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "sentences = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences into words\n",
    "tokenized_sentences = [word_tokenize(s.lower()) for s in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the vector for a specific word\n",
    "word_vector = model.wv['document']\n",
    "print(\"\\nWord2Vec vector for 'document':\\n\", word_vector)\n",
    "\n",
    "# You can also get vectors for all words in the vocabulary\n",
    "# print(\"\\nAll word vectors:\\n\", model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ede553-e9bc-4ca5-84b1-99578d5cff1b",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "+ gensim. models.Word2Vec is used to train the Word2Vec model.\n",
    "+ nltk.tokenize.word_tokenize is used to split sentences into individual words.\n",
    "+ The Word2Vec model is trained on the tokenized sentences.\n",
    "+ model.wv['word'] allows retrieving the vector representation for a specific word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8ce2964-4c42-443d-8ff1-32ccddfa604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conceptual embedding for 'document':\n",
      " [0.4 0.5 0.6]\n"
     ]
    }
   ],
   "source": [
    "# 3. Pre-trained Embeddings (Example with GloVe):\n",
    "import numpy as np\n",
    "\n",
    "# This is a conceptual example as downloading and loading large pre-trained models\n",
    "# would require more setup. You would typically load a file like 'glove.6B.100d.txt'\n",
    "\n",
    "# Example of how you might load and use pre-trained embeddings:\n",
    "# def load_glove_embeddings(filepath):\n",
    "#     embeddings_index = {}\n",
    "#     with open(filepath, encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             coefs = np.asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs\n",
    "#     return embeddings_index\n",
    "\n",
    "# embeddings = load_glove_embeddings('path/to/glove.6B.100d.txt')\n",
    "\n",
    "# To get an embedding for a word:\n",
    "# word_to_find = 'document'\n",
    "# if word_to_find in embeddings:\n",
    "#     embedding_vector = embeddings[word_to_find]\n",
    "#     print(f\"\\nGloVe embedding for '{word_to_find}':\\n\", embedding_vector)\n",
    "# else:\n",
    "#     print(f\"\\n'{word_to_find}' not found in GloVe embeddings.\")\n",
    "\n",
    "# For demonstration, creating a dummy embedding dictionary\n",
    "dummy_embeddings = {\n",
    "    'this': np.array([0.1, 0.2, 0.3]),\n",
    "    'document': np.array([0.4, 0.5, 0.6]),\n",
    "    'is': np.array([0.7, 0.8, 0.9])\n",
    "}\n",
    "word_to_find = 'document'\n",
    "if word_to_find in dummy_embeddings:\n",
    "    embedding_vector = dummy_embeddings[word_to_find]\n",
    "    print(f\"\\nConceptual embedding for '{word_to_find}':\\n\", embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35edf7e6-531f-40ba-9413-75114529d4c0",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "+ This section provides a conceptual outline for using pre-trained embeddings.\n",
    "+ It typically involves loading a large file containing word-vector mappings (e.g., GloVe files).\n",
    "+ Once loaded, you can look up the vector for any word present in the pre-trained vocabulary. The example uses a small dummy dictionary for illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd4b1a-5e28-4d57-b434-b978f536a112",
   "metadata": {},
   "source": [
    "# ● Textual features:\n",
    "# o Word count, character count, average word length, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04a726-463e-4e2a-8706-a44902d86125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment  \\\n",
      "0      One of the other reviewers has mentioned that ...  positive   \n",
      "1      A wonderful little production. <br /><br />The...  positive   \n",
      "2      I thought this was a wonderful way to spend ti...  positive   \n",
      "3      Basically there's a family where a little boy ...  negative   \n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "...                                                  ...       ...   \n",
      "49995  I thought this movie did a down right good job...  positive   \n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
      "49997  I am a Catholic taught in parochial elementary...  negative   \n",
      "49998  I'm going to have to disagree with the previou...  negative   \n",
      "49999  No one expects the Star Trek movies to be high...  negative   \n",
      "\n",
      "       word_count  character_count  average_word_length  \n",
      "0             307             1455             4.739414  \n",
      "1             162              837             5.166667  \n",
      "2             166              761             4.584337  \n",
      "3             138              611             4.427536  \n",
      "4             230             1088             4.730435  \n",
      "...           ...              ...                  ...  \n",
      "49995         194              815             4.201031  \n",
      "49996         112              531             4.741071  \n",
      "49997         230             1051             4.569565  \n",
      "49998         212             1023             4.825472  \n",
      "49999         129              550             4.263566  \n",
      "\n",
      "[50000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data from the image\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\india\\\\Downloads\\\\Imdb data.csv\")\n",
    "\n",
    "# Calculate textual features\n",
    "df['word_count'] = df['review'].apply(lambda x: len(str(x).split()))\n",
    "df['character_count'] = df['review'].apply(lambda x: len(str(x).replace(\" \", \"\"))) # Exclude spaces\n",
    "df['average_word_length'] = df.apply(\n",
    "    lambda row: row['character_count'] / row['word_count'] if row['word_count'] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(df[['review', 'sentiment', 'word_count', 'character_count', 'average_word_length']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29feb5ce-fd5b-4a6c-8c06-4e8602bf3812",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "1. Calculate Word Count = A new column word_count, is added by applying a lambda function to the 'review' column. This function splits each review string by spaces and counts the number of resulting words.\n",
    "2. Calculate Character Count = The character_count column is created by calculating the length of each review string after removing spaces, ensuring only actual characters are counted.\n",
    "3. Calculate Average Word Length = The average_word_length is calculated by dividing the character_count by the word_count for each review. A check is included to prevent division by zero if a review has no words.\n",
    "# Print Results:\n",
    "   + Finally, the DataFrame is printed, displaying the original reviews and sentiments along with the newly calculated textual features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fcf36-496d-4c26-a2c3-553a920ff947",
   "metadata": {},
   "source": [
    "#  3. Model Development:\n",
    "#  ● Build and train classification models to predict the sentiment of reviews.\n",
    "#    o Experiment with various classification algorithms such as Logistic Regression, Naive Bayes, Support Vector Machine (SVM), Random Forest, and Neural Networks (e.g., LSTM, BERT, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856c167-e595-4407-ab15-ab00d2a721df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sentiment Analysis Model Training and Evaluation ---\n",
      "\n",
      "Training Logistic Regression Model:\n",
      "Logistic Regression Accuracy: 0.90\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89      4961\n",
      "    positive       0.89      0.91      0.90      5039\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "\n",
      "Training Naive Bayes Model:\n",
      "Naive Bayes Accuracy: 0.85\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85      4961\n",
      "    positive       0.85      0.85      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "\n",
      "Training Support Vector Machine (SVM) Model:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Prepare Data\n",
    "# In a real scenario, you'd load a dataset (e.g., from a CSV file)\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\india\\\\Downloads\\\\Imdb data.csv\")\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "# 2. Feature Extraction (TF-IDF Vectorization)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Limiting features for simplicity\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Model Development and Training\n",
    "\n",
    "print(\"--- Sentiment Analysis Model Training and Evaluation ---\")\n",
    "\n",
    "# a) Logistic Regression\n",
    "print(\"\\nTraining Logistic Regression Model:\")\n",
    "lr_model = LogisticRegression(max_iter=1000) # Increase max_iter for convergence\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, lr_predictions):.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, lr_predictions))\n",
    "\n",
    "# b) Naive Bayes (Multinomial Naive Bayes for text data)\n",
    "print(\"\\nTraining Naive Bayes Model:\")\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, nb_predictions):.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, nb_predictions))\n",
    "\n",
    "# c) Support Vector Machine (SVM)\n",
    "print(\"\\nTraining Support Vector Machine (SVM) Model:\")\n",
    "svm_model = SVC(kernel='linear') # Linear kernel often works well for text\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, svm_predictions):.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, svm_predictions))\n",
    "\n",
    "# d) Random Forest\n",
    "print(\"\\nTraining Random Forest Model:\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_predictions):.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, rf_predictions))\n",
    "\n",
    "# Optional: Placeholder for Deep Learning (e.g., LSTM, BERT) - requires more complex setup\n",
    "print(\"\\nNote: Implementing Neural Networks (e.g., LSTM, BERT) would require libraries like TensorFlow or PyTorch and a more extensive code setup, including handling sequential data for LSTMs or using pre-trained models for BERT.\")\n",
    "\n",
    "# 4. Example of using a trained model for prediction\n",
    "new_reviews = [\"This movie was great!\", \"I hated every minute of it.\", \"It was just okay, nothing special.\"]\n",
    "new_reviews_tfidf = tfidf_vectorizer.transform(new_reviews)\n",
    "\n",
    "print(\"\\n--- Predicting Sentiment for New Reviews (using Logistic Regression) ---\")\n",
    "for i, review in enumerate(new_reviews):\n",
    "    predicted_sentiment = lr_model.predict(new_reviews_tfidf[i])\n",
    "    print(f\"Review: '{review}' -> Predicted Sentiment: {predicted_sentiment[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62abd1db-7b16-4378-9827-484bb5ff195f",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "1. Data Preparation:\n",
    "               +  A sample DataFrame with reviews and their corresponding sentiments is created. In a real project, you would load your dataset (e.g., pd.read_csv('your_reviews.csv')).\n",
    "               + The data is split into features (text reviews) and the target variable (sentiment labels).\n",
    "2. Feature Extraction:\n",
    "             + Text data needs to be converted into numerical features for machine learning models to process.\n",
    "             + TfidfVectorizer converts text documents to a matrix of TF-IDF features, representing the importance of words in the context of the entire corpus.\n",
    "3. Model Development and Training:\n",
    "       + The data is split into training and testing sets to evaluate model performance on unseen data.\n",
    "       + Logistic Regression: A linear model used for classification, suitable for binary and multiclass sentiment classification.\n",
    "       + Multinomial Naive Bayes: A probabilistic classifier often used for text classification tasks, particularly effective with word count or TF-IDF features.\n",
    "      + Support Vector Machine (SVM): A supervised learning algorithm effective for both linear and non-linear classification, commonly used in text classification.\n",
    "      + Random Forest: An ensemble method that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting.\n",
    "    + Each model is trained using the fit method on the training data and then evaluated using accuracy_score and classification_report on the test set.\n",
    "4. Deep Learning (Note):\n",
    "     + Neural Networks like LSTM or BERT require specialized libraries (TensorFlow, PyTorch, Hugging Face Transformers) and a more complex setup for sequence handling and leveraging pre-trained models. The provided code focuses on traditional machine learning models but acknowledges the role of deep learning in advanced sentiment analysis.\n",
    "\n",
    "# CONCLUSION :\n",
    "This code provides a foundational approach to building and training sentiment analysis models using various classification algorithms in Python. Remember to adapt the data loading, preprocessing, and model parameters to suit your specific dataset and requirements for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f4ceb-fcf5-46e3-b69d-b8bcd557a525",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation:\n",
    "#  ● Evaluate the model’s performance using appropriate metrics.\n",
    "Evaluating a model's performance in Python involves using various metrics depending on the type of model (e.g., classification, regression). Below is an example of Python code for evaluating a classification model, along with explanations of common metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ff1ee7-117e-46db-90c9-5dfbf5ace9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5333\n",
      "Precision: 0.3846\n",
      "Recall: 0.4545\n",
      "F1-Score: 0.4167\n",
      "ROC AUC Score: 0.5263\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11  8]\n",
      " [ 6  5]]\n"
     ]
    }
   ],
   "source": [
    "# Classification Model Evaluation:\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.random.rand(100, 5) # 100 samples, 5 features\n",
    "y = np.random.randint(0, 2, 100) # 100 binary labels (0 or 1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a simple classification model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
    "\n",
    "# Evaluate using various metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee35025-c911-4b19-b288-4679942a8b92",
   "metadata": {},
   "source": [
    "# Explanation of Metrics:\n",
    "# 1. Accuracy:\n",
    "   + Definition: The proportion of correctly classified instances out of the total instances.\n",
    "   + Explanation: It's a general measure of how well the model performs across all classes.\n",
    "   + Formula: \\(\\frac{\\text{Number\\ of\\ correct\\ predictions}}{\\text{Total\\ number\\ of\\ predictions}}\\)\n",
    "# 2. Precision:\n",
    "  + Definition: The proportion of true positive predictions among all positive predictions made by the model.\n",
    "  + Explanation: Useful when the cost of false positives is high (e.g., spam detection).\n",
    "  + Formula: \\(\\frac{\\text{True\\ Positives}}{\\text{True\\ Positives}+\\text{False\\ Positives}}\\)\n",
    "# 3. Recall (Sensitivity):\n",
    "  + Definition: The proportion of true positive predictions among all actual positive instances.\n",
    "  +  Explanation: Useful when the cost of false negatives is high (e.g., disease detection).\n",
    "  +   Formula: \\(\\frac{\\text{True\\ Positives}}{\\text{True\\ Positives}+\\text{False\\ Negatives}}\\)\n",
    "# 4. F1-Score:\n",
    "  + Definition: The harmonic mean of Precision and Recall.\n",
    "  +  Explanation: Provides a balance between Precision and Recall, especially useful when there's an uneven class distribution.\n",
    "  +   Formula: \\(2\\times \\frac{\\text{Precision}\\times \\text{Recall}}{\\text{Precision}+\\text{Recall}}\\)\n",
    "# 5. ROC AUC Score (Receiver Operating Characteristic - Area Under the Curve):\n",
    "  + Definition: Measures the ability of a classifier to distinguish between classes.\n",
    "  + Explanation: A higher AUC indicates a better model performance in separating positive and negative classes. It's particularly useful for imbalanced datasets.\n",
    "# 6. Confusion Matrix:\n",
    " + Definition: A table that summarizes the performance of a classification model on a set of test data.\n",
    " + Explanation: It shows the number of true positives, true negatives, false positives, and false negatives, providing a detailed breakdown of correct and incorrect predictions for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79daed0-3c53-4c49-93bd-3b1514131ae5",
   "metadata": {},
   "source": [
    "# Success Criteria:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15a3f7-aa94-4c76-9cec-e2785f5eeff0",
   "metadata": {},
   "source": [
    "# 1. Model Performance:\n",
    "     Achieve acceptable performance on test data using metrics like accuracy, F1-score, and ROC-AUC for the classification model.    \n",
    "# 2. Sentiment Insights:\n",
    "     Clearly communicate insights into factors influencing sentiment, such as word frequency and review length.\n",
    "# 3. Prediction Accuracy:\n",
    "     Make predictions for new movie reviews with a reasonable degree of accuracy.\n",
    "# 4. Effective Communication:\n",
    "     Ensure the final presentation effectively communicates the project's results and analysis.\n",
    "# 5. Clear Visualizations:\n",
    "     Utilize plots like bar charts, confusion matrices, and word clouds to clearly present data trends and model results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622cd1e5-c36f-46b5-8bee-acf13064b2f4",
   "metadata": {},
   "source": [
    "# Tools Required:\n",
    "Python Libraries:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
